{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c26b63-3fbd-4cef-a2d3-dadc2d078f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "b2.get_file('modules/auxiliary_functions.py')\n",
    "!pip install 'powerlaw'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c922ee8d-8c90-4e71-9cc0-607439699fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import auxiliary_functions as af\n",
    "import bmll2 as b2\n",
    "\n",
    "import random\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import StringDtype\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib.ticker import LogFormatterSciNotation\n",
    "\n",
    "from statsmodels.sandbox.stats.runs import runstest_1samp \n",
    "import powerlaw\n",
    "import itertools\n",
    "import pylab\n",
    "import scipy.stats\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc9d8f2-7f77-4444-81f4-4ac06cf6ba49",
   "metadata": {},
   "outputs": [],
   "source": [
    "b2.get_file('top100_tickers.csv')\n",
    "top100_tickers = pd.read_csv('top100_tickers.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39e72cc-60c9-437b-a4cc-58440baa65e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "top100_tickers = top100_tickers['0'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87adf09-e06f-41a3-bd53-d2c4f2972ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = b2.list_files(path = 'top_100(Volume)')\n",
    "\n",
    "for f in files:\n",
    "    b2.get_file(f'top_100(Volume)/{f}')\n",
    "    \n",
    "def load_stock(csv):\n",
    "    df = pd.read_csv(csv, parse_dates = ['DateTime', 'Date'])\n",
    "    df = df.rename(columns = {'Ticker': 'RIC'})\n",
    "    return df\n",
    "    \n",
    "stocks = {\n",
    "    ticker: load_stock(f'{ticker}.csv')\n",
    "    for ticker in top100_tickers\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8370b3af-2226-4a6b-9508-fa5de6367a10",
   "metadata": {},
   "source": [
    "## Finding metaorder run lengths $L$\n",
    "\n",
    "The functions used to extract the run lengths $L$ of all the metaorders is given below. We have used a one sided runs test instead of a two sided runs test because a trader can only be classified as a splitting-trader if their trades exhibit fewer runs than expected. While having more runs than expected is indicative of non random trading, these types of trader are perhaps better described as scalpers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7fc742-9398-4e10-aa1f-8d216f235da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_sided_runs_test(sample, runs_correction = 0):\n",
    "    \n",
    "    sample = pd.Series(sample)\n",
    "    values = sample.unique()\n",
    "\n",
    "    if (len(values) < 2):\n",
    "        return 'Only 1 unique value present'\n",
    "        \n",
    "    a = values[0]\n",
    "    b = values[1]\n",
    "\n",
    "    N_a  = len(sample[sample == a])\n",
    "    N_b  = len(sample[sample == b])\n",
    "    N    = N_a + N_b\n",
    "    mu   = ((2 * N_a * N_b) / N) + 1\n",
    "    runs = itertools.groupby(sample)\n",
    "    R    = sum(1 for _ in runs)\n",
    "    R_corrected = R + runs_correction\n",
    "\n",
    "    sigma = np.sqrt((2 * N_a * N_b * (2 * N_a * N_b - N)) / (N ** 2 * (N - 1)))\n",
    "    z     = (R_corrected - mu) / sigma\n",
    "\n",
    "    p_value = scipy.stats.norm.cdf(z)\n",
    "\n",
    "    return (z, p_value, R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f97195d-6e25-4dad-91f9-6dfc93c06dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ST_run_lengths(stock_data, N = 100, trader_distribution = 'power', alpha = 2, p_threshold = 0.01):\n",
    "\n",
    "    run_lengths = []\n",
    "    f = af.trader_participation(N = N, method = trader_distribution, alpha =  alpha, f_min = 1 ,f_max = stock_data.shape[0], seed = 1)\n",
    "    c = af.cumulative_probs(f)\n",
    "    output = af.orders(N = N, trades = stock_data, cumulative_probs = c)\n",
    "\n",
    "    for n in range(N):\n",
    "        trader_trades = stock_data.iloc[output[n]]\n",
    "\n",
    "        # Too few trades → useless\n",
    "        if trader_trades.shape[0] <= 2:\n",
    "            continue\n",
    "\n",
    "        # No sign variation → no runs test\n",
    "        if trader_trades['Trade Sign'].nunique() < 2:\n",
    "            continue\n",
    "\n",
    "        # Correct runs across day boundaries\n",
    "        day_breaks = 0\n",
    "        prev_date  = None\n",
    "        prev_sign  = None\n",
    "\n",
    "        for idx, row in trader_trades.iterrows():\n",
    "            if prev_date is not None:\n",
    "                if row['Date'] != prev_date and row['Trade Sign'] == prev_sign:\n",
    "                    day_breaks += 1\n",
    "            prev_date = row['Date']\n",
    "            prev_sign = row['Trade Sign']\n",
    "\n",
    "        z, p_val, R = one_sided_runs_test(trader_trades['Trade Sign'], runs_correction = day_breaks)\n",
    "\n",
    "        # Identify splitting traders\n",
    "        if p_val <= p_threshold:\n",
    "            for key, group in itertools.groupby(trader_trades['Trade Sign']):\n",
    "                run_lengths.append(len(list(group)))\n",
    "\n",
    "    return run_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2206c62-a0be-4df4-b974-c5234c52d717",
   "metadata": {},
   "outputs": [],
   "source": [
    "N                   = 200\n",
    "trader_distribution = 'power' # or homogenous\n",
    "alpha               = 2\n",
    "identifier          = f'{trader_distribution}_{N}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef222349-f65b-473d-b8e2-510518cd1ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "stocks_trade_sequences    = [] # will be a 3 deep list. 40 stocks, all days, < 10 sequences per day, indexes of trades for ST traders\n",
    "stocks_p_vals             = [] # will be a 3 deep list. 40 stocks, all days, < 10 p vals per day, a single value per ST trader\n",
    "stocks_run_lengths        = [] # will be a 3 deep list. 40 stocks, all days, < 10 traders per day, run lengths of each metaorder\n",
    "stocks_total_volumes      = [] # will be a 2 deep list, 40 stocks, all days, total volume traded per day of each stock\n",
    "stocks_percentage_STs     = [] \n",
    "stocks_percentage_STs_vol = []\n",
    "stocks_percentage_STs_num = []\n",
    "\n",
    "for ric, stock_data in stocks.items():\n",
    "\n",
    "    stock_data['Date'] = pd.to_datetime(stock_data['Date'])\n",
    "    stock_data['Year'] = stock_data['Date'].dt.year\n",
    "\n",
    "    for year, df_year in stock_data.groupby('Year'):\n",
    "    \n",
    "        trader_trade_sequences      = [] # will be a 2 deep list. all days, < 10 sequences per day, indexes of trades for ST traders\n",
    "        trader_p_vals               = [] # will be a 2 deep list. all days, < 10 p vals per day, a single value per ST trader\n",
    "        trader_run_lengths          = [] # will be a 2 deep list. all days, < 10 traders per day, run lengths of each metaorder\n",
    "        trader_total_volumes        = [] # will be a 1 deep list. all days, total volume traded per day\n",
    "        trader_percentage_STs       = []\n",
    "        trader_percentage_STs_vol   = []\n",
    "        trader_percentage_STs_num   = []\n",
    "    \n",
    "        f = af.trader_participation(N = N, method = trader_distribution, alpha = alpha, f_min = 1, f_max = stock_data.shape[0], seed = 1)\n",
    "        c = af.cumulative_probs(f)\n",
    "    \n",
    "        if stock_data.empty:\n",
    "            continue\n",
    "    \n",
    "        output = af.orders(N = N, trades = df_year, cumulative_probs = c)\n",
    "        \n",
    "        STs_volume = 0\n",
    "        STs_num    = 0\n",
    "        for n in range(N):\n",
    "    \n",
    "            trader_n_trades = df_year.iloc[output[n], ]\n",
    "    \n",
    "            if (trader_n_trades.shape[0] <= 2):\n",
    "                continue\n",
    "    \n",
    "            if (trader_n_trades['Trade Sign'].nunique() < 2):\n",
    "                continue\n",
    "    \n",
    "            day_breaks = 0\n",
    "            prev_date  = None\n",
    "            prev_sign  = None\n",
    "            \n",
    "            for idx, row in trader_n_trades.iterrows():\n",
    "                current_date = row['Date']\n",
    "                current_sign = row['Trade Sign']\n",
    "                \n",
    "                # If we've moved to a new day and the sign is THE SAME from end of previous day\n",
    "                if prev_date is not None and current_date != prev_date and current_sign == prev_sign:\n",
    "                    day_breaks += 1\n",
    "                \n",
    "                prev_date = current_date\n",
    "                prev_sign = current_sign\n",
    "    \n",
    "            runs_test = one_sided_runs_test(trader_n_trades['Trade Sign'], runs_correction = day_breaks)\n",
    "            p_val     = runs_test[1]\n",
    "    \n",
    "            # we need to decide on an appropriate p value here. 1% seems too strict\n",
    "            if p_val <= 0.01:\n",
    "    \n",
    "                STs_volume = STs_volume + sum(trader_n_trades['Volume'])\n",
    "                STs_num    = STs_num + trader_n_trades.shape[0]\n",
    "                trader_trade_sequences.append(trader_n_trades)\n",
    "                trader_p_vals.append(p_val)\n",
    "                \n",
    "                grouped_trade_signs = itertools.groupby(trader_n_trades['Trade Sign'])\n",
    "                \n",
    "                for key, group in grouped_trade_signs:\n",
    "                    trader_run_lengths.append(len(list(group)))\n",
    "    \n",
    "            trader_total_volumes.append(sum(stock_data['Volume']))\n",
    "\n",
    "        STs_percentage_vol = round((STs_volume / sum(stock_data['Volume'])) * 100, 3)\n",
    "        STs_percentage_num = round((STs_num / stock_data.shape[0]) * 100, 3)\n",
    "        \n",
    "        stocks_trade_sequences.append(trader_trade_sequences)\n",
    "        stocks_p_vals.append(trader_p_vals)\n",
    "        stocks_run_lengths.append(trader_run_lengths)\n",
    "        stocks_total_volumes.append(trader_total_volumes)\n",
    "        stocks_percentage_STs.append(round((len(trader_trade_sequences) / N) * 100, 3))\n",
    "        stocks_percentage_STs_vol.append(STs_percentage_vol)\n",
    "        stocks_percentage_STs_num.append(STs_percentage_num)\n",
    "        print('done with', ric, 'for', year)\n",
    "\n",
    "\n",
    "stocks_percentage_STs     = np.array(stocks_percentage_STs)\n",
    "stocks_percentage_STs_vol = np.array(stocks_percentage_STs_vol)\n",
    "stocks_percentage_STs_num = np.array(stocks_percentage_STs_num)\n",
    "\n",
    "STs_percentage = stocks_percentage_STs[stocks_percentage_STs > 0]\n",
    "STs_vols       = stocks_percentage_STs_vol[stocks_percentage_STs_vol > 0]\n",
    "STs_nums       = stocks_percentage_STs_num[stocks_percentage_STs_num > 0]\n",
    "\n",
    "ST_df          = pd.DataFrame({'% STs' : STs_percentage, 'STs volume' : STs_vols, 'STs number' : STs_nums})\n",
    "ST_df.to_csv(f'ST_df_{identifier}.csv', index = False)\n",
    "b2.put_file(f'ST_df_{identifier}.csv', 'test_data')   \n",
    "\n",
    "# takes about 4 hours to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c66232f-fd5f-42e9-a942-919071034897",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for ticker, stock_data in stocks.items():\n",
    "\n",
    "    stock_data['Date'] = pd.to_datetime(stock_data['Date'])\n",
    "    stock_data['Year'] = stock_data['Date'].dt.year\n",
    "\n",
    "    for year, df_year in stock_data.groupby('Year'):\n",
    "\n",
    "        if df_year.shape[0] < 1000:\n",
    "            continue  # too small for power laws\n",
    "\n",
    "        L     = extract_ST_run_lengths(df_year, N = N, trader_distribution = trader_distribution, alpha = alpha)\n",
    "        signs = df_year['Trade Sign']\n",
    "        \n",
    "        if len(L) == 0:\n",
    "            continue\n",
    "        \n",
    "        out = pd.DataFrame({'L' : L})\n",
    "        out.to_csv(f'{ticker}_{trader_distribution}_{N}_run_lengths_yearly_{year}.csv', index = False)\n",
    "        b2.put_file(f'{ticker}_{trader_distribution}_{N}_run_lengths_yearly_{year}.csv', 'test_data')\n",
    "\n",
    "        signs = pd.DataFrame({'Trade Sign' : signs})\n",
    "        signs.to_csv(f'{ticker}_{trader_distribution}_{N}_trade_signs_{year}.csv', index = False)\n",
    "        b2.put_file(f'{ticker}_{trader_distribution}_{N}_trade_signs_{year}.csv', 'test_data')\n",
    "        \n",
    "    print(f'saved yearly run lengths for {ticker}')\n",
    "\n",
    "# takes about 2 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918e69bf-82c6-4f35-9092-0f82adf181a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "files = b2.list_files(path = 'test_data')\n",
    "\n",
    "files = [f for f in files if (f'{identifier}_run_lengths' in f) or (f'{identifier}_trade_signs' in f)]\n",
    "for f in files:\n",
    "    b2.get_file(f'test_data/{f}')\n",
    "\n",
    "# takes about 2 minutes to load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e7598e-87e8-45df-85e3-419e87d4472d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "yearly_run_lengths_2023 = {}\n",
    "yearly_run_lengths_2024 = {}\n",
    "yearly_run_lengths_2025 = {}\n",
    "\n",
    "for ticker in top100_tickers:\n",
    "\n",
    "    try:\n",
    "        yearly_run_lengths_2023[ticker] = pd.read_csv(f'{ticker}_{identifier}_run_lengths_yearly_2023.csv')\n",
    "    except FileNotFoundError:\n",
    "        print (f'{ticker}_{identifier}_run_lengths_yearly_2023.csv not found')\n",
    "        pass\n",
    "        \n",
    "    try:\n",
    "        yearly_run_lengths_2024[ticker] = pd.read_csv(f'{ticker}_{identifier}_run_lengths_yearly_2024.csv')\n",
    "    except FileNotFoundError:\n",
    "        print (f'{ticker}_{identifier}_run_lengths_yearly_2024.csv not found')\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        yearly_run_lengths_2025[ticker] = pd.read_csv(f'{ticker}_{identifier}_run_lengths_yearly_2025.csv')\n",
    "    except FileNotFoundError:\n",
    "        print (f'{ticker}_{identifier}_run_lengths_yearly_2025.csv not found')\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eea5316-e057-43f0-be1d-b556e4efd4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "trade_signs_2023 = {}\n",
    "trade_signs_2024 = {}\n",
    "trade_signs_2025 = {}\n",
    "\n",
    "for ticker in top100_tickers:\n",
    "\n",
    "    try:\n",
    "        trade_signs_2023[ticker] = pd.read_csv(f'{ticker}_{identifier}_trade_signs_2023.csv')\n",
    "    except FileNotFoundError:\n",
    "        print (f'{ticker}_{identifier}_trade_signs_2023.csv not found')\n",
    "        pass\n",
    "        \n",
    "    try:\n",
    "        trade_signs_2024[ticker] = pd.read_csv(f'{ticker}_{identifier}_trade_signs_2024.csv')\n",
    "    except FileNotFoundError:\n",
    "        print (f'{ticker}_{identifier}_trade_signs_2024.csv not found')\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        trade_signs_2025[ticker] = pd.read_csv(f'{ticker}_{identifier}_trade_signs_2025.csv')\n",
    "    except FileNotFoundError:\n",
    "        print (f'{ticker}_{identifier}trade_signs_2025.csv not found')\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7df68e-c322-4826-abee-a6d4a8329081",
   "metadata": {},
   "source": [
    "## Functions used to estimate values for $\\alpha$ and $\\gamma$\n",
    "\n",
    "The functions used to estimate the values of $\\alpha$ and $\\gamma$ from the trade signs $\\epsilon$ and metaorder run lengths $L$ are given below. The NLLS estimate of $\\gamma$ was used in this iteration of the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d55032b-8081-43fa-b5ce-62367bb431ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sign_autocorrelation(signs, max_lag):\n",
    "    \n",
    "    signs = np.asarray(signs)\n",
    "    N = len(signs)\n",
    "\n",
    "    taus = np.arange(1, max_lag + 1)\n",
    "    C = np.empty(max_lag)\n",
    "\n",
    "    for i, tau in enumerate(taus):\n",
    "        C[i] = np.mean(signs[:-tau] * signs[tau:])\n",
    "\n",
    "    return taus, C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d010017-59b1-48eb-bd8f-4be4862185e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_powerlaw_range_fast(taus, C, min_points = 10000):\n",
    "\n",
    "    mask = (C > 0) & np.isfinite(C)\n",
    "    taus, C = taus[mask], C[mask]\n",
    "\n",
    "    log_tau = np.log(taus)\n",
    "    log_C   = np.log(C)\n",
    "\n",
    "    N = len(log_tau)\n",
    "\n",
    "    # cumulative sums\n",
    "    # x is the log taus\n",
    "    # y is the log C\n",
    "    Sx  = np.cumsum(log_tau)\n",
    "    Sy  = np.cumsum(log_C)\n",
    "    Sxx = np.cumsum(log_tau ** 2)\n",
    "    Syy = np.cumsum(log_C ** 2)\n",
    "    Sxy = np.cumsum(log_tau * log_C)\n",
    "\n",
    "    best_r2 = -np.inf\n",
    "    best_i  = None\n",
    "\n",
    "    j = N  # fixed right endpoint (tail fit)\n",
    "\n",
    "    for i in range(N - min_points):\n",
    "        \n",
    "        n = j - i\n",
    "\n",
    "        # gonna be working backwards here. We already precomputed the cumulative sum of the entire series we just need to subtract\n",
    "        # the part thats no longer in it. It starts off on the largest window and makes it smaller each time. With this method we fix\n",
    "        # the left endpoint of the window so that we only need to do a O(N) search and not a O(N^2) search. \n",
    "        sum_x  = Sx[j - 1]  - (Sx[i - 1]  if i > 0 else 0)\n",
    "        sum_y  = Sy[j - 1]  - (Sy[i - 1]  if i > 0 else 0)\n",
    "        sum_xx = Sxx[j - 1] - (Sxx[i - 1] if i > 0 else 0)\n",
    "        sum_yy = Syy[j - 1] - (Syy[i - 1] if i > 0 else 0)\n",
    "        sum_xy = Sxy[j - 1] - (Sxy[i - 1] if i > 0 else 0)\n",
    "\n",
    "        denom = n * sum_xx - sum_x ** 2\n",
    "        if denom == 0:\n",
    "            continue\n",
    "\n",
    "        slope = ((n * sum_xy) - (sum_x * sum_y)) / denom\n",
    "        intercept = (sum_y - (slope * sum_x)) / n\n",
    "\n",
    "        ss_tot = sum_yy - (sum_y ** 2)/n\n",
    "        ss_res = sum_yy + (slope ** 2) * sum_xx + (n * intercept ** 2) - (2 * slope * sum_xy) - (2 * intercept * sum_y) + (2 * slope * intercept * sum_x)\n",
    "\n",
    "        r2 = 1 - (ss_res / ss_tot) if ss_tot > 0 else -np.inf\n",
    "\n",
    "        if r2 > best_r2:\n",
    "            best_r2 = r2\n",
    "            best_i  = i\n",
    "            best_slope = slope\n",
    "\n",
    "    return best_i, N - 1, best_slope\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1125d6f-5745-4c1e-832c-7b6f37209b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def powerlaw_fit(tau, C_0, gamma):\n",
    "    \n",
    "    return C_0 * (tau ** (- gamma))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ccd1f5-b936-4d26-a79f-470aa6e05f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_gamma_loglog(taus, C):\n",
    "\n",
    "    x = np.log(taus)\n",
    "    y = np.log(C)\n",
    "\n",
    "    slope, intercept = np.polyfit(x, y, 1)\n",
    "    gamma = -slope\n",
    "    C_0   = np.exp(intercept)\n",
    "\n",
    "    return C_0, gamma\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7fe9e2-bfe7-4746-b379-0c78175e5d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_gamma(taus, C, min_points, nbins):\n",
    "\n",
    "    mask = (C > 0) & (np.isfinite(C))\n",
    "    taus = taus[mask]\n",
    "    C    = C[mask]\n",
    "    \n",
    "    i, j, slope     = select_powerlaw_range_fast(taus, C, min_points)\n",
    "    gamma_nlls_init = -1 * slope\n",
    "\n",
    "    tau_min = taus[i]\n",
    "    tau_max = taus[j]\n",
    "\n",
    "    mask_fit = (taus >= tau_min) & (taus <= tau_max) & (np.isfinite(C))\n",
    "    taus     = taus[mask_fit]\n",
    "    C        = C[mask_fit]\n",
    "\n",
    "    C_0_init        = C[0] if len(C) > 0 else 1.0\n",
    "    C_0, gamma_nlls = estimate_gamma_loglog(taus, C)\n",
    "\n",
    "    return {'C_0' : C_0, 'gamma_nlls' : gamma_nlls}#, 'taus_smooth' : taus_smooth, 'C_smoth' : C_smooth}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4830f1ab-e84a-4b45-846b-e32fdb658261",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_alpha(run_lengths):\n",
    "    \n",
    "    fit = powerlaw.Fit(run_lengths, discrete = True)\n",
    "    \n",
    "    # The powerlaw package gives alpha for CCDF P_>(L) ~ L^(-alpha)\n",
    "    # For PDF P(L) ~ L^(-alpha-1), we have the same alpha\n",
    "    alpha = fit.alpha - 1  # Adjust for PDF vs CCDF convention\n",
    "    xmin  = fit.xmin\n",
    "    \n",
    "    return alpha, xmin\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421c5578-a505-4842-84c6-4805946950a3",
   "metadata": {},
   "source": [
    "## Estimating $\\alpha$ and $\\gamma$\n",
    "\n",
    "In the next section of the notebook, the saved data is used to estimate the values of $\\alpha$ and $\\gamma$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4900fa-3ebd-4207-a4ff-4e414902105e",
   "metadata": {},
   "outputs": [],
   "source": [
    "N                   = 200\n",
    "trader_distribution = 'power'\n",
    "alpha               = 2\n",
    "identifier          = f'{trader_distribution}_{N}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538e2b8b-e695-4ecf-8190-7c862854847c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "results = []\n",
    "\n",
    "for year, L_dict, trade_dict in [\n",
    "    (2023, yearly_run_lengths_2023, trade_signs_2023),\n",
    "    (2024, yearly_run_lengths_2024, trade_signs_2024),\n",
    "    (2025, yearly_run_lengths_2025, trade_signs_2025),\n",
    "]:\n",
    "    print(f'\\033[91mProcessing year: {year}\\033[0m')\n",
    "    for ticker in top100_tickers:\n",
    "        print(f'{ticker}')\n",
    "        try:\n",
    "            # Try to get the trade signs and run lengths\n",
    "            signs = trade_dict[ticker]\n",
    "            L     = L_dict[ticker]['L']\n",
    "\n",
    "            # Calculate autocorrelation and gamma\n",
    "            taus, C = sign_autocorrelation(signs, max_lag = 10000)\n",
    "            out     = estimate_gamma(taus, C, min_points = 1000, nbins = 10)\n",
    "\n",
    "            # Estimate alpha\n",
    "            alpha, xmin = estimate_alpha(L)\n",
    "\n",
    "            # Append results\n",
    "            results.append({'year' : year, 'ticker' : ticker, 'xmin' : xmin, 'alpha' : alpha, 'gamma_nlls' : out['gamma_nlls']})\n",
    "\n",
    "        except KeyError as e:\n",
    "            print(f'Warning: missing data for {ticker} in {year}, skipping... ({e})')\n",
    "        except Exception as e:\n",
    "            print(f'Warning: error processing {ticker} in {year}, skipping... ({e})')\n",
    "\n",
    "alpha_gamma_df = pd.DataFrame(results)\n",
    "\n",
    "alpha_gamma_df.to_csv(f'alpha_gamma_df_{identifier}.csv', index = False)\n",
    "b2.put_file(f'alpha_gamma_df_{identifier}.csv', 'test_data')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311-stable",
   "language": "python",
   "name": "py311-stable"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
